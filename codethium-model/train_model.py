# -*- coding: utf-8 -*-
"""model1-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11mjMGf70j0tuBBCzIR5bZGQg7WReFNB2
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader,IterableDataset
import torch.nn.functional as F
import sentencepiece as spm
import numpy as np
import json
import os
import math
import random
from typing import List, Tuple, Optional
from tqdm import tqdm
import matplotlib.pyplot as plt
from pathlib import Path
import matplotlib.pyplot as plt
from IPython import display
display.set_matplotlib_formats('svg')

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
device

class Attention(nn.Module):
  def __init__(self,d_model,n_heads,dropout=0.1,max_seq_len=512):
    super().__init__()
    assert d_model % n_heads ==0

    self.d_model = d_model
    self.n_heads = n_heads
    self.d_k = d_model//n_heads

    self.c_atten = nn.Linear(d_model,3*d_model)
    self.c_proj = nn.Linear(d_model,d_model)

    self.dropout = nn.Dropout(dropout)
    self.resid_dropout = nn.Dropout(dropout)

    self.register_buffer("casual_mask",torch.tril(torch.ones(max_seq_len,max_seq_len)).view(1,1,max_seq_len,max_seq_len))

    self.scale = 1.0/math.sqrt(self.d_k)

  def forward(self,x):
    batch_size,seq_len,d_model = x.size()

    qkv = self.c_atten(x)

    q,k,v = qkv.split(d_model,dim=2)

    q = q.view(batch_size,seq_len,self.n_heads,self.d_k).transpose(1,2)
    k = k.view(batch_size,seq_len,self.n_heads,self.d_k).transpose(1,2)
    v = v.view(batch_size,seq_len,self.n_heads,self.d_k).transpose(1,2)

    attn_scores = torch.matmul(q,k.transpose(-2,-1))*self.scale

    casual_mask = self.casual_mask[:,:,:seq_len,:seq_len]

    attn_scores = attn_scores.masked_fill(casual_mask==0,float('-inf'))

    attn_weights = F.softmax(attn_scores,dim=-1)
    attn_weights = self.dropout(attn_weights)

    attn = torch.matmul(attn_weights,v)

    attn = attn.transpose(1,2).contiguous().view(batch_size,seq_len,d_model)

    out = self.resid_dropout(self.c_proj(attn))
    return out

class PositionalEncoding(nn.Module):
  def __init__(self,d_model,max_seq_len=512):
    super().__init__()
    pe = torch.zeros(max_seq_len,d_model)
    position = torch.arange(0,max_seq_len).unsqueeze(1).float()

    div_term = torch.exp(torch.arange(0,d_model,2).float()* -math.log(10000)/d_model)

    pe[:,0::2] = torch.sin(position*div_term)
    pe[:,1::2] = torch.cos(position*div_term)

    self.register_buffer('pe',pe.unsqueeze(0))

  def forward(self,x):
    return x + self.pe[:,:x.size(1)]

class FeedForward(nn.Module):
  def __init__(self,d_model,dropout=0.1):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(d_model,4*d_model),
        nn.GELU(),
        nn.Linear(4*d_model,d_model),
        nn.Dropout(dropout)
    )
  def forward(self,x):
    return self.net(x)

class Block(nn.Module):
  def __init__(self,d_model,n_heads,max_seq_len=512,dropout=0.1):
    super().__init__()
    self.ln1 = nn.LayerNorm(d_model)
    self.attn = Attention(d_model,n_heads,dropout,max_seq_len)
    self.ln2 = nn.LayerNorm(d_model)
    self.ff = FeedForward(d_model,dropout)
  def forward(self,x):
    x = x + self.attn(self.ln1(x))
    x = x + self.ff(self.ln2(x))
    return x

from ast import mod
class tinyTransformerDecoderOnly(nn.Module):
  def __init__(self,vocab_size,d_model,n_heads,n_layers,max_seq_len=512,dropout=0.1):
    super().__init__()
    self.max_seq_len = max_seq_len
    self.tok_emb = nn.Embedding(vocab_size,d_model)
    self.pos_emb = PositionalEncoding(d_model,max_seq_len)
    self.blocks = nn.Sequential(*[Block(d_model,n_heads,max_seq_len,dropout) for _ in range(n_layers)])
    self.ln_f = nn.LayerNorm(d_model)
    self.head = nn.Linear(d_model,vocab_size,bias=False)
    self.apply(self._init_weights)
  def _init_weights(self,module):
    if isinstance(module,(nn.Linear,nn.Embedding)):
      nn.init.normal_(module.weight,mean=0.0,std=0.02)
    if isinstance(module,nn.Linear) and module.bias is not None:
      nn.init.zeros_(module.bias)

  def forward(self,idx,targets=None):
    batch_size,seq_len = idx.size()
    assert seq_len <= self.max_seq_len
    x = self.tok_emb(idx)
    x = self.pos_emb(x)
    x = self.blocks(x)
    x = self.ln_f(x)
    logits = self.head(x)
    loss=None
    if targets is not None:
      loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))

    return logits,loss

data_extend = [
  {"text": "Return the middle element of a list if length is odd, else return the two middle elements",
   "code": "def middle_elements(lst):\n    n = len(lst)\n    if n % 2 == 1:\n        return lst[n//2]\n    return [lst[n//2 - 1], lst[n//2]]"},

  {"text": "Flatten a nested list of integers into a single list",
   "code": "def flatten(lst):\n    result = []\n    for sub in lst:\n        result.extend(sub)\n    return result"},

  {"text": "Group words in a list by their first letter",
   "code": "from collections import defaultdict\n\ndef group_by_first_letter(words):\n    groups = defaultdict(list)\n    for w in words:\n        groups[w[0]].append(w)\n    return dict(groups)"},

  {"text": "Count how many unique words appear in a sentence",
   "code": "def unique_word_count(sentence):\n    return len(set(sentence.split()))"},

  {"text": "Rotate a list to the right by k steps",
   "code": "def rotate_right(lst, k):\n    k %= len(lst)\n    return lst[-k:] + lst[:-k]"},

  {"text": "Merge two dictionaries, summing values of common keys",
   "code": "def merge_dicts(d1, d2):\n    result = d1.copy()\n    for k, v in d2.items():\n        result[k] = result.get(k, 0) + v\n    return result"},

  {"text": "Find all indices of a value in a list",
   "code": "def find_indices(lst, val):\n    return [i for i, x in enumerate(lst) if x == val]"},

  {"text": "Check if two strings are anagrams ignoring case",
   "code": "def is_anagram(a, b):\n    return sorted(a.lower()) == sorted(b.lower())"},

  {"text": "Return the most common character in a string",
   "code": "from collections import Counter\n\ndef most_common_char(s):\n    return Counter(s).most_common(1)[0][0]"},

  {"text": "Remove consecutive duplicate characters from a string",
   "code": "def remove_consecutive_duplicates(s):\n    result = [s[0]]\n    for c in s[1:]:\n        if c != result[-1]:\n            result.append(c)\n    return ''.join(result)"},

  {"text": "Chunk a list into sublists of length n",
   "code": "def chunk_list(lst, n):\n    return [lst[i:i+n] for i in range(0, len(lst), n)]"},

  {"text": "Transpose a matrix represented as a list of lists",
   "code": "def transpose(matrix):\n    return [list(row) for row in zip(*matrix)]"},

  {"text": "Compute dot product of two equal-length lists",
   "code": "def dot_product(a, b):\n    return sum(x*y for x,y in zip(a,b))"},

  {"text": "Remove all dictionary keys with None values",
   "code": "def remove_none(d):\n    return {k:v for k,v in d.items() if v is not None}"},

  {"text": "Find the second largest number in a list",
   "code": "def second_largest(lst):\n    unique = sorted(set(lst))\n    return unique[-2] if len(unique) >= 2 else None"},

  {"text": "Check if a list is strictly increasing",
   "code": "def is_strictly_increasing(lst):\n    return all(lst[i] < lst[i+1] for i in range(len(lst)-1))"},

  {"text": "Return the filename extension from a file path",
   "code": "import os\n\ndef file_extension(path):\n    return os.path.splitext(path)[1]"},

  {"text": "Convert a list of (key, value) pairs into a dictionary",
   "code": "def pairs_to_dict(pairs):\n    return dict(pairs)"},

  {"text": "Capitalize the first letter of each word in a sentence",
   "code": "def capitalize_words(s):\n    return ' '.join(w.capitalize() for w in s.split())"},

  {"text": "Check if all elements in a list are unique",
   "code": "def all_unique(lst):\n    return len(lst) == len(set(lst))"},

  {"text": "Count frequency of each word in a sentence ignoring case",
   "code": "from collections import Counter\n\ndef word_frequencies(sentence):\n    words = sentence.lower().split()\n    return dict(Counter(words))"},

  {"text": "Find the longest word in a list",
   "code": "def longest_word(words):\n    return max(words, key=len)"},

  {"text": "Generate a dictionary mapping numbers to their squares up to n",
   "code": "def squares_dict(n):\n    return {i: i*i for i in range(1, n+1)}"},

  {"text": "Replace spaces in a string with underscores",
   "code": "def spaces_to_underscores(s):\n    return s.replace(' ', '_')"},

  {"text": "Compute the sum of digits of an integer",
   "code": "def sum_of_digits(n):\n    return sum(int(c) for c in str(abs(n)))"},

  {"text": "Remove duplicates from a list while preserving order",
   "code": "def unique_preserve_order(lst):\n    seen = set()\n    result = []\n    for x in lst:\n        if x not in seen:\n            seen.add(x)\n            result.append(x)\n    return result"},

  {"text": "Return the last n elements of a list",
   "code": "def last_n(lst, n):\n    return lst[-n:]"},

  {"text": "Swap the keys and values in a dictionary",
   "code": "def invert_dict(d):\n    return {v: k for k, v in d.items()}"},

  {"text": "Check if a number is a perfect square",
   "code": "import math\n\ndef is_perfect_square(n):\n    root = int(math.sqrt(n))\n    return root*root == n"},

  {"text": "Extract all digits from a string",
   "code": "def extract_digits(s):\n    return ''.join(c for c in s if c.isdigit())"},

  {"text": "Count uppercase letters in a string",
   "code": "def count_uppercase(s):\n    return sum(1 for c in s if c.isupper())"},

  {"text": "Check if all characters in a string are alphabetic",
   "code": "def all_alpha(s):\n    return s.isalpha()"},

  {"text": "Find intersection of two lists",
   "code": "def list_intersection(a, b):\n    return list(set(a) & set(b))"},

  {"text": "Return the nth Fibonacci number using iteration",
   "code": "def fibonacci(n):\n    a, b = 0, 1\n    for _ in range(n):\n        a, b = b, a + b\n    return a"},

  {"text": "Count words longer than 3 characters in a sentence",
   "code": "def count_long_words(sentence):\n    return sum(1 for w in sentence.split() if len(w) > 3)"},

  {"text": "Remove punctuation from a string",
   "code": "import string\n\ndef remove_punctuation(s):\n    return s.translate(str.maketrans('', '', string.punctuation))"},

  {"text": "Return the index of the first occurrence of a substring",
   "code": "def find_substring(s, sub):\n    return s.find(sub)"},

  {"text": "Check if a year is a leap year",
   "code": "def is_leap_year(year):\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)"},

  {"text": "Remove all even numbers from a list",
   "code": "def remove_evens(lst):\n    return [x for x in lst if x % 2 != 0]"},

  {"text": "Check if a string contains only unique characters",
   "code": "def unique_chars(s):\n    return len(set(s)) == len(s)"},

  {"text": "Count the number of words in a string",
   "code": "def word_count(s):\n    return len(s.split())"},

  {"text": "Return the product of all elements in a list",
   "code": "def product(lst):\n    result = 1\n    for x in lst:\n        result *= x\n    return result"},

  {"text": "Check if a number is odd",
   "code": "def is_odd(n):\n    return n % 2 != 0"},

  {"text": "Reverse the words in a sentence",
   "code": "def reverse_words(sentence):\n    return ' '.join(sentence.split()[::-1]"},

  {"text": "Return the minimum element in a list",
   "code": "def min_in_list(lst):\n    return min(lst)"},

  {"text": "Return the maximum element in a list",
   "code": "def max_in_list(lst):\n    return max(lst)"},

  {"text": "Check if two sets are disjoint in python ",
   "code": "def disjoint(a, b):\n    return len(set(a) & set(b)) == 0"},

  {"text": "Square all numbers in a list",
   "code": "def square_list(lst):\n    return [x*x for x in lst]"},

  {"text": "Remove whitespace from both ends of a string",
   "code": "def trim(s):\n    return s.strip()"},
  {"text": "Write function to sum two integer in python",
   "code": "def sum(a,b):\n    return a+b"}

]

def loadDataSet():
  try:
    from datasets import load_dataset
  except Exception as e:
    raise RuntimeError("Install `datasets` (pip install datasets) to download MBPP automatically.") from e

  ds_train = load_dataset('mbpp',split='train')
  ds_validation = load_dataset('mbpp',split='validation')
  ds_test = load_dataset('mbpp',split='test')

  ds = list(ds_train)+list(ds_validation)+list(ds_test)
  ds.extend(data_extend)
  return ds

def loadDataSet_local(path):
  path = Path(path)
  if not path.exists():
    raise FileExistsError(f'{path} is not exist ')
  data = []
  if path.suffix == '.jsonl':
    with open(path,'r',encoding='utf-8') as f:
      for line in f:
        data.append(json.loads(line))
  else:
    with open(path,'r',encoding='utf=8') as f:
      data.append(json.load(f))
  return data

special_tokens = ["<PAD>","<BOS>","<EOS>","<UNK>","<NL>","<INDENT>","<DEDENT>"]

def format_example(prompt,program):
  text = '<BOS> # problem: ' + prompt.strip() + ' <NL> ' + program.replace('\r\n','<NL>').strip() + ' <EOS>'
  return text

def prepare_dataset_items(mbpp_record):
  examples = []

  for example in mbpp_record:
    prompt = example.get('text')
    program = example.get('code')
    if not prompt or not program:
      continue
    examples.append(format_example(prompt,program))
  return examples

def train_full_sentencepiece_model(text_examples,model_prefix='fullspm',vocab_size=5000,model_type='bpe'):
  try:
    import sentencepiece as spm
  except Exception as e:
    raise RuntimeError('Install sentencepiece (pip install sentencepiece) to use full SPM pipeline.') from e

  tmp = model_prefix + '_corpus.txt'
  with open(tmp,'w',encoding='utf-8') as f:
    for line in text_examples:
      f.write(line.replace('\n',' ')+'\n')
  user_symbols = ','.join(special_tokens)
  cmd = (
        f"--input={tmp} --model_prefix={model_prefix} --vocab_size={vocab_size} "
        f"--model_type={model_type} --unk_id=0 --pad_id=1 --bos_id=-1 --eos_id=-1 "
        f"--user_defined_symbols={user_symbols}"
    )
  spm.SentencePieceTrainer.Train(cmd)
  sp = spm.SentencePieceProcessor()
  model_file= f"{model_prefix}.model"
  if not os.path.exists(model_file):
    raise FileNotFoundError(f"SentencePiece model file {model_file} was not created.")
  sp.load(model_file)
  return sp

def spm_tokenize_text(sp_processor,text):
  return sp_processor.encode(text,out_type=str)

def spm_detokenize_pieces(sp_processor,pieces):
  try:
    return sp_processor.decode_pieces(pieces)
  except Exception:
    return "".join(pieces)

class Vocab:
  def __init__(self,sp_processor):
    self.sp = sp_processor
    self.id2token = self.sp.get_piece_size() and [self.sp.id_to_piece(i) for i in range(self.sp.get_piece_size())] or []
    self.token2id = {t: i for i,t in enumerate(self.id2token)}
    for t in special_tokens:
      if t not in self.token2id:
        self.token2id[t] = len(self.id2token)
        self.id2token.append(t)
  def encode(self,pieces):
    return [self.token2id.get(p,self.token2id.get("<UNK>")) for  p in pieces]
  def decode(self,id):
    return [self.id2token[i] if 0<= i < len(self.id2token) else '<ERR>' for i in id]

  def size(self):
    return len(self.id2token)

def build_token_stream_fullspm(examples,sp_processor=None,max_rep=1):
  all_piece_lists = []
  for ex in examples:
    pieces = spm_tokenize_text(sp_processor,ex) if sp_processor else ex.split()
    all_piece_lists.append(pieces)
  vocab = Vocab(sp_processor)
  long_ids = []

  for pieces in all_piece_lists:
    ids = vocab.encode(pieces)
    long_ids.extend(ids)
  long_ids = long_ids*max(1,max_rep)
  return long_ids,vocab

class StreamDataset(IterableDataset):
  def __init__(self,token_stream,max_len):
    self.tokens = torch.tensor(token_stream,dtype=torch.long)
    self.max_len = max_len
    self.actual_len = len(self.tokens)
  def __iter__(self):
    while True:
      if self.actual_len <= self.max_len+1:
        start=0
      else:
        start = random.randint(0,max(0,self.actual_len-self.max_len-1))
      chunk = self.tokens[start:start+self.max_len+1]
      x = chunk[:-1].clone()
      y = chunk[1:].clone()
      yield x,y

@torch.no_grad()
def sample_logits_to_id(logits,top_k=0,top_p=0.0,temperature=1.0):
  logits =logits/(temperature if temperature>0 else 1)
  if top_k>0:
    topk_vals,topk_idx = torch.topk(logits,top_k)
    min_topk= topk_vals[-1]
    logits = torch.where(logits<min_topk,torch.tensor(-1e9,device=logits.device),logits)
  if top_p >0.0:
    sorted_logits,sorted_idx = torch.sort(logits,descending=True)
    probs_logits = F.softmax(sorted_logits,dim=-1)
    cumulative_probs_logits = torch.cumsum(probs_logits,dim=-1)
    cutoff = (cumulative_probs_logits>top_p).nonzero(as_tuple=False)
    if cutoff.numel()>0:
      idx_cut = cutoff[0,0].item()
      thresh= sorted_logits[idx_cut]
      logits  = torch.where(logits<thresh, torch.tensor(-1e10,device=logits.device),logits)

  probs = F.softmax(logits,dim=-1)
  next_id = torch.multinomial(probs,num_samples=1)
  return int(next_id.item())

def generate(model, vocab, sp_processor, prompt, max_new_tokens=200, top_k=50, top_p=0.9, temperature=0.9, device='cpu'):
  model.eval()
  pieces = spm_tokenize_text(sp_processor,prompt)
  ids = vocab.encode(pieces)
  x = torch.tensor(ids,dtype=torch.long,device= device).unsqueeze(0)
  for _ in range(max_new_tokens):
    x_cond = x[:,-model.max_seq_len:]
    logits,_ = model(x_cond)
    last_logits = logits[0,-1,:].clone()
    next_id = sample_logits_to_id(last_logits,top_k,top_p,temperature)
    x = torch.cat([x,torch.tensor([[next_id]],device=device)],dim=1)
    if next_id<vocab.size() and vocab.id2token[next_id]=='<EOS>':
      break

  toks_out = vocab.decode(x[0].tolist())
  output = spm_detokenize_pieces(sp_processor,toks_out)
  return output


def main(
    use_hf= True,max_len=512,max_rep=4,local_path =None,
    fullspm_size = 5000,sp_model_prefix = 'fullspm',
    model_cfg = dict(n_layers = 4,n_hesads=4,d_model =256),
    epoches = 800,batchs=8,lr = 1e-4,accum=2,device=None
):
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  if use_hf and local_path is None:
    print('Loading full MBPP dataset (train, validation, test) via Hugging Face datasets (internet required)...')
    records =  loadDataSet()
  else :
    print('Loading MBPP from local path...')
    records = loadDataSet_local(local_path)

  examples = prepare_dataset_items(records)
  print(f'loaded examples: {len(examples)}')

  print("Training full-text SentencePiece (this may take a minute)...")
  sp = train_full_sentencepiece_model(examples,model_prefix=sp_model_prefix,vocab_size=fullspm_size,model_type='bpe')
  print("SPM model trained. Piece size:", sp.get_piece_size())
  long_ids,vocab = build_token_stream_fullspm(examples=examples,sp_processor=sp,max_rep=max_rep)
  print("Vocab size:", vocab.size(), "Long token stream length:", len(long_ids)) # Changed vocab.size() to vocab.size

  ds_stream = StreamDataset(long_ids,max_len=max_len)

  loader = DataLoader(ds_stream,batch_size=batchs,num_workers=0)

  model = tinyTransformerDecoderOnly(vocab_size=vocab.size(),d_model = model_cfg.get('d_model',256),n_heads= model_cfg.get('n_heads',4)
  ,n_layers=model_cfg.get('n_layers',4),max_sseq_len=max_len).to(device)

  print(f'Model Parameters: {sum(p.numel() for p in model.parameters())/1e6}')

  optimizer = torch.optim.AdamW(model.parameters(),lr=lr)

  model.train()

  it = iter(loader)
  # for x,y in it:
  #   print(x,y)
  ema_loss = None
  train_loss = []
  for step in range(1,epoches+1):
    optimizer.zero_grad()
    total_loss=0.0
    for _ in range(accum):
      x_batch,y_batch = next(it)
      x_batch = x_batch.to(device)
      y_batch = y_batch.to(device)

      logits,loss = model(x_batch,y_batch)
      (loss/accum).backward()
      total_loss += float(loss.item())
    torch.nn.utils.clip_grad_norm(model.parameters(),1.0)
    optimizer.step()
    ema_loss = total_loss if ema_loss is None else 0.98*ema_loss+0.02*total_loss
    train_loss.append(ema_loss)
    if step % 20 ==0 or step==1:
      print(f"Step {step}/{epoches} loss={total_loss:.4f} ema={ema_loss:.4f}")

  plt.plot(train_loss)
  model.eval()
  test_prompts = [
        "<BOS> # problem: function to reverse words in a given string in python. <NL> ",
        "<BOS> # problem: check if is a prime number <NL> ",
        "<BOS> # problem: Write a function sum a,b <NL> ",
        " Write a function to find the longest chain which can be formed from the given set of pairs."
    ]
  for p in test_prompts:
        out = generate(model, vocab, sp, p, max_new_tokens=200, top_k=50, top_p=0.9, temperature=0.9, device=device)
        print("PROMPT:", p)
        print(out)
        print("===")

  torch.save(model.state_dict(),'tiny_transformer.pth')
  print("Model saved to tiny_transformer.pth")

  torch.save(vocab,'vocab.pth')
  print("✅ Vocab saved to vocab.pth")


  print(f"✅ SentencePiece model saved as {sp_model_prefix}.model and {sp_model_prefix}.vocab")

if __name__ == "__main__":
    main(
        use_hf=True,      
        local_path=None,
        max_len=512,
        max_rep=6,
        fullspm_size=5500,
        sp_model_prefix="fullspm",
        model_cfg=dict(n_layers=4, n_heads=4, d_model=256),
        epoches=8000,  
        batchs=6,
        accum=2,
        lr=3e-4,
        device=None
    )

